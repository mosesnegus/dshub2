{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Clustering and K-Means\n",
    "\n",
    "![](https://snag.gy/kYWumd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='unsupervised'></a>\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "---\n",
    "\n",
    "> **Supervised --> Classification**: Create a model to predict to which group a point belongs.\n",
    "\n",
    "> **Unsupervised --> Clustering**: Find groups that already exist in the data.\n",
    "\n",
    "Until now, we haven't talked much about unsupervised learning. We use unsupervised methods when we don't have labeled data. There are no true targets to predict; we derive the likely categories from the structure in our data.\n",
    "\n",
    "| Pros | Cons |\n",
    "|---|---|\n",
    "| No labels. | Difficult to evaluate correctness without subject matter expertise. |\n",
    "| Few or no assumptions about data. | Scaling/normalization often required. |\n",
    "| Useful for subset/segmentation discovery. | Can be difficult to visualize. |\n",
    "| Great for broad insights. | Extremely difficult to tune. |\n",
    "| Many models available. | No obvious choice in many cases. |\n",
    "| Black magic. | Considered unconventional and unreliable. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to Clustering\n",
    "\n",
    "---\n",
    "\n",
    "### Helpful Uses for Clustering \n",
    "   - Finding items with similar behavior (users, products, voters, etc.).\n",
    "   - Market segmentation.\n",
    "   - Understanding complex systems.\n",
    "   - Discovering meaningful categories for your data.\n",
    "   - Reducing the number of classes by grouping (e.g., bourbons, scotches -> whiskeys).\n",
    "   - Reducing the dimensions of your problem.\n",
    "   - Pre-processing is important! Creating labels for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Genetics\n",
    "![](https://snag.gy/TP2RA4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consumer Internet\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://snag.gy/EbLeqd.jpg\"></td>\n",
    "        <td><img src=\"https://snag.gy/xsNvK8.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Business\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td><img src=\"https://snag.gy/pDueQ2.jpg\" width=\"500\"></td>\n",
    "    <td>\n",
    "         <li>Identifying demographics.</li>\n",
    "        <li>Spending patterns.</li>\n",
    "        <li>Consumer trends.</li>\n",
    "        <li>Customer characteristics.</li>\n",
    "        <li>Recommender systems.</li>\n",
    "        <li>Taxonomy/categorization.</li>\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Do We Mean by \"Labeled Data?\"\n",
    "- Give some examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://snag.gy/YUt5RO.jpg\" style=\"float: left; margin-right: 25px; width: 250px\">\n",
    "\n",
    "## What Problems Do You Think Arise During \"Clustering?\"\n",
    "\n",
    "Follow up:\n",
    "\n",
    "- How accurate do you think these are?\n",
    "- What kind of data are we talking about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='what'></a>\n",
    "## What is Clustering? \n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/BdfATE.jpg\" style=\"width: 500px\">\n",
    "\n",
    "Clustering is one of the most ubiquitous and widespread processes for assigning discrete structure to data. In clustering, we group observations together in a data set such that the members of a group are more similar to each other than they are to members of other groups. There are a wide variety of methods and criteria for performing this task.\n",
    "\n",
    "**Properties of clustering procedures:**\n",
    "\n",
    "- No \"true\" target/response to compare.\n",
    "- We apply structure to data quantitatively based on specific criteria.\n",
    "- Predictions of labels are based on the structure of the data.\n",
    "\n",
    "For example: Your employer gives you a data set of voter preferences from a local poll. They want you to figure out how these voters are grouping based on their preferences. The answer? Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"algos\"></a>\n",
    "## Clustering Algorithms \n",
    "\n",
    "---\n",
    "\n",
    "The are many different algorithms that can perform clustering when given a data set:\n",
    "\n",
    "- **K-means**: Mean centroids.\n",
    "- **Hierarchical**: Nested clusters created by merging or splitting successively.\n",
    "- **DBSCAN**: Density based.\n",
    "- **Affinity propagation**: Graph-based approach to let points \"vote\" on their preferred \"exemplar.\"\n",
    "- **Mean shift**: Can find the number of clusters.\n",
    "- **Spectral clustering**.\n",
    "- **Agglomerative clustering**: Suite of algorithms based on applying the same criteria/characteristics of one cluster to others.\n",
    "\n",
    "Today we're going to look at one of the algorithms: **k-means.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='k-means'></a>\n",
    "## K-Means Clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### K-means is the most popular clustering algorithm.\n",
    "\n",
    "K-means is one of the easier clustering methods to understand; other clustering techniques use some of the same assumptions on which k-means relies.\n",
    "\n",
    "- **K** is the number of clusters.\n",
    "- **Means** refers to the mean points of the K clusters.\n",
    "\n",
    "The number of clusters, $k$, is chosen in advance. The goal is to partition the data into sets such that the total sum of squared distances from each point to the mean point of the cluster is minimized.\n",
    "\n",
    "The algorithm takes your entire data set and iterates over its features and observations to determine clusters based around center points. These center points are known as **centroids**. \n",
    "\n",
    "### K-means iterative fitting\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create).\n",
    "2. Initialize k centroids (starting points) in your data.\n",
    "3. Create your clusters. Assign each point to the nearest centroid. \n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat Steps 3 and 4 until your centroids converge. \n",
    "\n",
    "> **Note:** Unfortunately, there's no formula to determine the absolute best number of $k$ clusters. Unsupervised learning is inherently subjective! We can, however, choose the \"best\" $k$ based on predetermined criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='euclidean'></a>\n",
    "## Refresher: Euclidean Distance\n",
    "\n",
    "---\n",
    "\n",
    "### $$ d(x_1, x_2) = \\sqrt{\\sum_{i=1}^N (x_{1i} - x_{2i})^2} $$\n",
    "\n",
    "**As an example, take two points:**\n",
    "\n",
    "- $x1 = (2, -1)$\n",
    "- $x2 = (-2, 2)$\n",
    "\n",
    "**The Euclidean distance between these two points is:**\n",
    "\n",
    "### $$\\begin{aligned}\n",
    "d(x1, x2) &= \\sqrt{ (2 - (-2))^2 + ((-1) - 2)^2 } \\\\\n",
    "d(x1, x2) &= \\sqrt{25} \\\\\n",
    "d(x1, x2) &= 5 \n",
    "\\end{aligned}$$\n",
    "\n",
    "**Using scikit-learn:**\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "X = np.array([[2, -1], [-2, 2]])\n",
    "print euclidean_distances(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='km-steps'></a>\n",
    "## K-Means: Step by Step\n",
    "\n",
    "---\n",
    "\n",
    "<table width=\"500\" cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/7haoS3.jpg\" style=\"width: 150px\"></td>\n",
    "   <td style=\"vertical-align: top; width: 400px;\"><br><b>Step 1.</b><br>We have data in an n-dimensional feature space (2-D, for example).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><br><b>Step 2.</b><br>Initialize K centroid (2 here).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 3.</b><br>Assign points to *closest* cluster based on _Euclidean distance_.<br><br>$\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$\n",
    "\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/NY1EeT.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 4.</b><br>Calculate mean of points assigned to centroid (2 here). Update new centroid positions to mean (i.e., geometric center).<br><br>$new\\ centroid\\ position= \\bar{x}, \\bar{y}$\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/tSfDZs.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 5.</b><br>Repeat Steps 3–4, updating class membership based on centroid distance.\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/BbIicn.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Fin.</b><br>Convergence is met once all points no longer change to a new class (defined by closest centroid distance).\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='vis'></a>\n",
    "## K-Means: A Visual Example\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/5hFXUA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='caveats'></a>\n",
    "## A Few K-Means Caveats\n",
    "\n",
    "---\n",
    "\n",
    "Nothing is perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='sensitive'></a>\n",
    "### K-means is sensitive to outliers.\n",
    "\n",
    "![](https://snag.gy/WFNMQY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='centroid-init'></a>\n",
    "### K-means Is sensitive to centroid initialization.\n",
    "\n",
    "![](https://snag.gy/5sigCD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how-many-k'></a>\n",
    "### How Many Ks?\n",
    "\n",
    "Sometimes it's obvious; sometimes it's not. What do you think?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/4rU39.png\"><br>1</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/gq28F.png\"><br>2</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"https://snag.gy/cWPgno.jpg\"><br>3</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='choose-k'></a>\n",
    "## Choosing K\n",
    "\n",
    "---\n",
    "\n",
    "There are different methods of initializing centroids. For instance:\n",
    "\n",
    "- Randomly.\n",
    "- Manually.\n",
    "- Special `k-means++` method in scikit-learn. (_This initializes the centroids to be generally distant from each other._)\n",
    "\n",
    "**Depending on your problem, you may find some of these are better than others.**\n",
    "\n",
    "> **Note:** Manual is recommended if you know your data well enough to see the clusters without much help but is rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='converge'></a>\n",
    "## A Note on K-Means Convergence\n",
    "\n",
    "---\n",
    "\n",
    "In general, k-means will converge to a solution and return a partition of k clusters, even if no natural clusters exist in the data. It's entirely possible – *common*, in fact – that the clusters don't mean anything at all. \n",
    "\n",
    "**Knowing your domain and data set is essential. Evaluating the clusters visually is a must (if possible).**\n",
    "\n",
    "> _\"Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times with different initializations of the centroids. One method to help address this issue is the `k-means++` initialization scheme, which has been implemented in scikit-learn (use the `init='kmeans++'` parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\"_ — [Scikit-Learn Clustering Guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "![](http://www.datamilk.com/kmeans_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kmeans-skl'></a>\n",
    "## K-Means in Scikit-Learn\n",
    "\n",
    "---\n",
    "\n",
    "Below, we will implement k-means using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans, k_means\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples    =  200, \n",
    "    centers      =  3, \n",
    "    n_features   =  2,\n",
    "    random_state =  0\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X, columns=['x', 'y'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"x\", y=\"y\", figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Take note that the latest version of k-means behaves a little differently now.\n",
    "model = KMeans(n_clusters=3, random_state=0).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After we fit our data, we can get our predicted labels from `model.labels_` and the center points`model.cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model.labels_\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "print(\"Predicted clusters to points: \", predicted)\n",
    "print(\"Location of centroids: \")\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df['predicted'] = predicted\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='verify'></a>\n",
    "### Visually Verifying Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "df.plot(x=\"x\", y=\"y\", kind=\"scatter\", color=df['predicted'], )#colormap='gist_rainbow', alpha=.7)\n",
    "plt.scatter(centroids[:,:1], centroids[:,1:], marker='o', s=150, alpha=.7, c=range(0,3), cmap='gist_rainbow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='sil'></a>\n",
    "\n",
    "## Metrics: Inertia and the Silhouette Coefficient\n",
    "\n",
    "---\n",
    "\n",
    "**Inertia**: The sum of squared errors for each cluster.\n",
    "\n",
    "- Low inertia = dense cluster.\n",
    "- Ranges from zero to very high values.\n",
    "\n",
    "$$\\sum_{j=0}^{n} (x_j - \\mu_i)^2$$\n",
    "Where $\\mu_i$ is a cluster centroid (k-means explicitly tries to minimize this).\n",
    "\n",
    "`.inertia_` is an attribute of scikit-learn's k-means models.\n",
    "\n",
    "**Silhouette coefficient**: The measure of how far apart clusters are from each other.\n",
    "\n",
    "- High silhouette score = clusters are well separated.\n",
    "- Ranges from negative one to one.\n",
    "\n",
    "The definition is a little involved but, intuitively, the score is based on how much closer data points are to their own clusters than to the nearest neighbor cluster.\n",
    "\n",
    "We can calculate this in scikit-learn with `metrics.silhouette_score(X_scaled, labels, metric='euclidean')`.\n",
    "- https://en.wikipedia.org/wiki/Silhouette_(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "score = silhouette_score(df, predicted, metric='euclidean')\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='pluto'></a>\n",
    "## Practice: Scikit-Learn and K-Means With \"Isotopic Composition Plutonium Batches\"\n",
    "\n",
    "---\n",
    "\n",
    "We have a nice [data dictionary](https://vincentarelbundock.github.io/Rdatasets/doc/cluster/pluton.html).\n",
    "\n",
    "    Pu238: The percentage of (238)Pu, always less than two percent.\n",
    "\n",
    "    Pu239: The percentage of (239)Pu, typically between 60 and 80 percent (from neutron capture of Uranium, (238)U).\n",
    "\n",
    "    Pu240: The percentage of the plutonium 240 isotope.\n",
    "\n",
    "    Pu241: The percentage of the plutonium 241 isotope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/pluton.csv\"\n",
    "# There is also a copy of the CSV file in the \"assets/datasets\" file.\n",
    "\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Use Pu239 and Pu240 as Our Features\n",
    "1) Select only columns \"Pu239\" and \"Pu240\" to use for our example.\n",
    "2) Plot to see how it looks.\n",
    "3) Initialize an instance of `KMeans` from `sklearn`.\n",
    "4) Fit our sliced DataFrame with `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Run k-means against our two features with three clusters.\n",
    "\n",
    "# 2. Assign clusters back to our DataFrame.\n",
    "\n",
    "# 3. Get our centroids.\n",
    "\n",
    "# 4. Plot the scatter of our points with calculated centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='scaling'></a>\n",
    "### Try Standardizing the Data First and See How It Affects the Cluster Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same as above on scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion: K-Means Trade-Offs\n",
    "\n",
    "---\n",
    "\n",
    "**K-means:**\n",
    "\n",
    "- Unsupervised clustering model.\n",
    "- Similar to KNN (but for clustering).\n",
    "- Iteratively finds labels given k.\n",
    "- Easy to implement in scikit-learn.\n",
    "- Sensitive to shape and scale of data.\n",
    "- Optimal k is hard to evaluate.\n",
    "\n",
    "---\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "| -- | -- |\n",
    "| K-means is popular because it's simple and computationally efficient. | However, K-means is highly scale dependent and isn't suitable for data of varying shapes and densities. |\n",
    "| Easy to see results/intuitive. | Evaluating results is more subjective, requiring much more human evaluation than other trusted metrics. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- Andrew Moore's computer science class at Carnegie Mellon contains good, static visualization with step-by-step details. His slide deck can be found online [here](http://www.cs.cmu.edu/~cga/ai-course/kmeans.pdf). He also links to more of his tutorials on the first page. \n",
    "- [Scikit-Learn Clustering Overview](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- [Scikit-Learn K-Means Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [Scikit-Learn Clustering Code — See `k_init` for Explanation of `k-means++` Centroid Selection](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py#L769)\n",
    "- [Clustering Tutorial](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/)\n",
    "- [Wikipedia's Deep Dive on Clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "\n",
    "\n",
    "**Some helpful Stack Exchange questions:**\n",
    "\n",
    "- http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust\n",
    "- http://stats.stackexchange.com/questions/174556/k-means-clustering-with-dummy-variables\n",
    "- http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
