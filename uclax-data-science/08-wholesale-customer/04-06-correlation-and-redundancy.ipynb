{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Redundancy\n",
    "\n",
    "I claim that there is correlation and redundancy in the `customer` table. What I mean by this is that some features are linear combinations of other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine redundancy by dropping a feature and seeing if the other features can predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "customers = pd.read_csv('Wholesale_customers_data.csv')\n",
    "customers.Region = customers.Region.astype('category')\n",
    "customers.Channel = customers.Channel.astype('category')\n",
    "customer_features = customers.select_dtypes([int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_2_for_feature(data,feature):\n",
    "    new_data = data.drop(feature, axis=1)\n",
    "    target = data[feature]\n",
    "\n",
    "    X_train, \\\n",
    "    X_test,  \\\n",
    "    y_train, \\\n",
    "    y_test = train_test_split(\n",
    "        new_data,target,test_size=0.25\n",
    "    )\n",
    "\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    regressor.fit(X_train,y_train)\n",
    "\n",
    "    score = regressor.score(X_test,y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_r_2_for_feature(customer_features,'Detergents_Paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:24} {}\".format(\"Delicatessen: \", calculate_r_2_for_feature(customer_features,'Delicatessen')))\n",
    "print(\"{:24} {}\".format(\"Degergents_paper: \", calculate_r_2_for_feature(customer_features,'Detergents_Paper')))\n",
    "print(\"{:24} {}\".format(\"Fresh: \", calculate_r_2_for_feature(customer_features,'Fresh')))\n",
    "print(\"{:24} {}\".format(\"Frozen: \", calculate_r_2_for_feature(customer_features,'Frozen')))\n",
    "print(\"{:24} {}\".format(\"Grocery: \", calculate_r_2_for_feature(customer_features,'Grocery')))\n",
    "print(\"{:24} {}\".format(\"Milk: \", calculate_r_2_for_feature(customer_features,'Milk')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is subject to randomness. There is randomness in my `train_test_split`. Let's do the whole thing many times and take the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_r2_for_feature(data, feature):\n",
    "    scores = []\n",
    "    for _ in range(100):\n",
    "        scores.append(calculate_r_2_for_feature(data, feature))\n",
    "        \n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:24} {}\".format(\"Delicatessen: \", mean_r2_for_feature(customer_features,'Delicatessen')))\n",
    "print(\"{:24} {}\".format(\"Detergents_Paper: \", mean_r2_for_feature(customer_features,'Detergents_Paper')))\n",
    "print(\"{:24} {}\".format(\"Fresh: \", mean_r2_for_feature(customer_features,'Fresh')))\n",
    "print(\"{:24} {}\".format(\"Frozen: \", mean_r2_for_feature(customer_features,'Frozen')))\n",
    "print(\"{:24} {}\".format(\"Grocery: \", mean_r2_for_feature(customer_features,'Grocery')))\n",
    "print(\"{:24} {}\".format(\"Milk: \", mean_r2_for_feature(customer_features,'Milk')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:24} {}\".format(\"Delicatessen: \", mean_r2_for_feature(customer_features,'Delicatessen')))\n",
    "print(\"{:24} {}\".format(\"Detergents_Paper: \", mean_r2_for_feature(customer_features,'Detergents_Paper')))\n",
    "print(\"{:24} {}\".format(\"Fresh: \", mean_r2_for_feature(customer_features,'Fresh')))\n",
    "print(\"{:24} {}\".format(\"Frozen: \", mean_r2_for_feature(customer_features,'Frozen')))\n",
    "print(\"{:24} {}\".format(\"Grocery: \", mean_r2_for_feature(customer_features,'Grocery')))\n",
    "print(\"{:24} {}\".format(\"Milk: \", mean_r2_for_feature(customer_features,'Milk')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "What does this tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the correlation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "sns.pairplot(customer_features, kind='reg')\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = customer_features.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, 0)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr, mask=mask, square=True, annot=True,\n",
    "                     cmap='RdBu', fmt='+.3f')\n",
    "    plt.xticks(rotation=45, ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1pct_1 = customer_features.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_stats = sample_1pct_1.describe().T\n",
    "samp_stats['skew'] = st.skew(sample_1pct_1)\n",
    "samp_stats['kurt'] = st.kurtosis(sample_1pct_1)\n",
    "samp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = customer_features.describe().T\n",
    "stats['skew'] = st.skew(customer_features)\n",
    "stats['kurt'] = st.kurtosis(customer_features)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MANY OF THE TOOLS WE WILL USE WILL ASSUME NORMAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are already familiar with standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z = \\frac{X-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "customer_sc = scaler.fit_transform(customer_features)\n",
    "customer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\n",
    "\n",
    "sc_stats = customer_features.describe().T\n",
    "sc_stats['skew'] = st.skew(customer_features)\n",
    "sc_stats['kurt'] = st.kurtosis(customer_features)\n",
    "display(stats)\n",
    "display(sc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_features.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_features[col], label=col)\n",
    "    plt.axvline(customer_features[col].mean(), c='red')\n",
    "    plt.axvline(customer_features[col].median(), c='black')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_sc_df.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_sc_df[col], label=col)\n",
    "    plt.axvline(customer_sc_df[col].mean(), c='red')\n",
    "    plt.axvline(customer_sc_df[col].median(), c='black')\n",
    "    plt.legend()\n",
    "    plt.xlim(-5,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MANY OF THE TOOLS WE WILL USE WILL ASSUME NORMAL DATA\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
